{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b87c2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7cc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efca1d",
   "metadata": {},
   "source": [
    "# Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9a49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"database/pdf_sections_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af39b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee8cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/pdf_sections_data.pkl', 'rb') as f:\n",
    "        sections_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1ba6a",
   "metadata": {},
   "source": [
    "# RAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182bdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=3):\n",
    "    query_vector = model.encode([query])[0].astype('float32')\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'distance': dist,\n",
    "            'content': sections_data[idx]['content'],\n",
    "            'metadata': sections_data[idx]['metadata']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67edc46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/5727mf8x4yb4knqc1wx3w4kw0000gn/T/ipykernel_14683/2553128167.py:16: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n",
      "/var/folders/jd/5727mf8x4yb4knqc1wx3w4kw0000gn/T/ipykernel_14683/2553128167.py:21: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def answer_question(query):\n",
    "    # Search for relevant context\n",
    "    search_results = search_faiss(query)\n",
    "    \n",
    "    # Combine the content from the search results\n",
    "    context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b176af9",
   "metadata": {},
   "source": [
    "# Reading GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab68dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/QA_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7e22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/var/folders/jd/5727mf8x4yb4knqc1wx3w4kw0000gn/T/ipykernel_14683/2553128167.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(context=context, question=query)\n",
      "100%|██████████| 10/10 [01:48<00:00, 10.83s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(df))):\n",
    "    query = df['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b327e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latency'] = time_list\n",
    "df['response'] = response_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c147204",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d799e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = Ollama(\n",
    "    model=\"phi3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f788dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['correctness', 'relevance', 'coherence', 'conciseness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ec2b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:25<00:00, 14.50s/it]\n",
      "100%|██████████| 10/10 [02:16<00:00, 13.66s/it]\n",
      "100%|██████████| 10/10 [02:03<00:00, 12.39s/it]\n",
      "100%|██████████| 10/10 [02:36<00:00, 15.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    evaluator = load_evaluator(\"labeled_criteria\", criteria=metric, llm=eval_llm)\n",
    "    \n",
    "    reasoning = []\n",
    "    value = []\n",
    "    score = []\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "            prediction=df.response.values[i],\n",
    "            input=df.Questions.values[i],\n",
    "            reference=df.Answers.values[i]\n",
    "        )\n",
    "        reasoning.append(eval_result['reasoning'])\n",
    "        value.append(eval_result['value'])\n",
    "        score.append(eval_result['score'])\n",
    "    \n",
    "    df[metric+'_reasoning'] = reasoning\n",
    "    df[metric+'_value'] = value\n",
    "    df[metric+'_score'] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7797a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correctness_score     0.875000\n",
       "relevance_score       0.888889\n",
       "coherence_score       0.900000\n",
       "conciseness_score     0.600000\n",
       "latency              10.831184\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['correctness_score','relevance_score','coherence_score','conciseness_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe667926",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q=pd.read_csv('data/irrelevant_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "189f8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:47<00:00, 10.73s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(irr_q))):\n",
    "    query = irr_q['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0244ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q['response']=response_list\n",
    "irr_q['latency']=time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "debd3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q['irrelevant_score'] = irr_q['response'].str.contains(\"I don't know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bef1d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "irrelevant_score     0.800000\n",
       "latency             10.728183\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irr_q[['irrelevant_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1610a70",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff6614f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question concisely. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=new_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def answer_question_new(query):\n",
    "    # Search for relevant context\n",
    "    search_results = search_faiss(query)\n",
    "    \n",
    "    # Combine the content from the search results\n",
    "    context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20580d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1b3d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:18<00:00, 13.83s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(df2))):\n",
    "    query = df2['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63f41256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['latency'] = time_list\n",
    "df2['response'] = response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d8a6065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [15:45<00:00, 94.55s/it]\n",
      "100%|██████████| 10/10 [06:05<00:00, 36.59s/it]\n",
      "100%|██████████| 10/10 [03:09<00:00, 18.99s/it]\n",
      "100%|██████████| 10/10 [08:28<00:00, 50.88s/it]\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    evaluator = load_evaluator(\"labeled_criteria\", criteria=metric, llm=eval_llm)\n",
    "    \n",
    "    reasoning = []\n",
    "    value = []\n",
    "    score = []\n",
    "    \n",
    "    for i in tqdm(range(len(df2))):\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "            prediction=df2.response.values[i],\n",
    "            input=df2.Questions.values[i],\n",
    "            reference=df2.Answers.values[i]\n",
    "        )\n",
    "        reasoning.append(eval_result['reasoning'])\n",
    "        value.append(eval_result['value'])\n",
    "        score.append(eval_result['score'])\n",
    "    \n",
    "    df2[metric+'_reasoning'] = reasoning\n",
    "    df2[metric+'_value'] = value\n",
    "    df2[metric+'_score'] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d1002b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correctness_score     0.500000\n",
       "relevance_score       0.000000\n",
       "coherence_score       1.000000\n",
       "conciseness_score     1.000000\n",
       "latency              13.829776\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['correctness_score','relevance_score','coherence_score','conciseness_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808bdcf",
   "metadata": {},
   "source": [
    "# Query relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b541f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_search_faiss(query, k=3, threshold=0.5):\n",
    "    query_vector = model.encode([query])[0].astype('float32')\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if dist < threshold:  # Only include results within the threshold distance\n",
    "            results.append({\n",
    "                'distance': dist,\n",
    "                'content': sections_data[idx]['content'],\n",
    "                'metadata': sections_data[idx]['metadata']\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f579654",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question concisely. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def new_answer_question(query):\n",
    "    # Search for relevant context\n",
    "    search_results = new_search_faiss(query)\n",
    "    \n",
    "    if search_results==[]:\n",
    "        response=\"I don't know\"\n",
    "    else:\n",
    "        context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "        response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f83ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2=irr_q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f06474e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 28.13it/s]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(irr_q2))):\n",
    "    query = irr_q['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = new_answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52db6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2['response']=response_list\n",
    "irr_q2['latency']=time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4508de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2['irrelevant_score'] = irr_q2['response'].str.contains(\"I don't know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d34ba06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "irrelevant_score    1.000000\n",
       "latency             0.035431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irr_q2[['irrelevant_score','latency']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interviewer Project",
   "language": "python",
   "name": "interviewer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
