{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b87c2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8a7cc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.evaluation import load_evaluator\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efca1d",
   "metadata": {},
   "source": [
    "# Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cc9a49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"database/pdf_sections_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9af39b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fee8cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/pdf_sections_data.pkl', 'rb') as f:\n",
    "        sections_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1ba6a",
   "metadata": {},
   "source": [
    "# RAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "182bdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=3):\n",
    "    query_vector = model.encode([query])[0].astype('float32')\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'distance': dist,\n",
    "            'content': sections_data[idx]['content'],\n",
    "            'metadata': sections_data[idx]['metadata']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "67edc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def answer_question(query):\n",
    "    # Search for relevant context\n",
    "    search_results = search_faiss(query)\n",
    "    \n",
    "    # Combine the content from the search results\n",
    "    context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b176af9",
   "metadata": {},
   "source": [
    "# Reading GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4ab68dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/QA_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4e7e22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:13<00:00,  7.35s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(df))):\n",
    "    query = df['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2b327e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latency'] = time_list\n",
    "df['response'] = response_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c147204",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d799e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = Ollama(\n",
    "    model=\"phi3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f788dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['correctness', 'relevance', 'coherence', 'conciseness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "83ec2b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:11<00:00,  7.18s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:50<00:00,  5.02s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:49<00:00,  4.94s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [03:02<00:00, 18.29s/it]\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    evaluator = load_evaluator(\"labeled_criteria\", criteria=metric, llm=eval_llm)\n",
    "    \n",
    "    reasoning = []\n",
    "    value = []\n",
    "    score = []\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "            prediction=df.response.values[i],\n",
    "            input=df.Questions.values[i],\n",
    "            reference=df.Answers.values[i]\n",
    "        )\n",
    "        reasoning.append(eval_result['reasoning'])\n",
    "        value.append(eval_result['value'])\n",
    "        score.append(eval_result['score'])\n",
    "    \n",
    "    df[metric+'_reasoning'] = reasoning\n",
    "    df[metric+'_value'] = value\n",
    "    df[metric+'_score'] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7797a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correctness_score    1.000000\n",
       "relevance_score      0.800000\n",
       "coherence_score      0.800000\n",
       "conciseness_score    0.888889\n",
       "latency              7.344954\n",
       "dtype: float64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['correctness_score','relevance_score','coherence_score','conciseness_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fe667926",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q=pd.read_csv('data/irrelevant_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "189f8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:55<00:00,  5.50s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(irr_q))):\n",
    "    query = irr_q['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b0244ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q['response']=response_list\n",
    "irr_q['latency']=time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "debd3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q['irrelevant_score'] = irr_q['response'].str.contains(\"I don't know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "bef1d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "irrelevant_score    1.000000\n",
       "latency             5.498948\n",
       "dtype: float64"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irr_q[['irrelevant_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1610a70",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ff6614f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question concisely. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=new_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def answer_question_new(query):\n",
    "    # Search for relevant context\n",
    "    search_results = search_faiss(query)\n",
    "    \n",
    "    # Combine the content from the search results\n",
    "    context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "20580d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b1b3d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:04<00:00,  6.46s/it]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(df2))):\n",
    "    query = df2['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "63f41256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['latency'] = time_list\n",
    "df2['response'] = response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0d8a6065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:55<00:00,  5.54s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:56<00:00,  5.61s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:34<00:00,  3.47s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:37<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    evaluator = load_evaluator(\"labeled_criteria\", criteria=metric, llm=eval_llm)\n",
    "    \n",
    "    reasoning = []\n",
    "    value = []\n",
    "    score = []\n",
    "    \n",
    "    for i in tqdm(range(len(df2))):\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "            prediction=df2.response.values[i],\n",
    "            input=df2.Questions.values[i],\n",
    "            reference=df2.Answers.values[i]\n",
    "        )\n",
    "        reasoning.append(eval_result['reasoning'])\n",
    "        value.append(eval_result['value'])\n",
    "        score.append(eval_result['score'])\n",
    "    \n",
    "    df2[metric+'_reasoning'] = reasoning\n",
    "    df2[metric+'_value'] = value\n",
    "    df2[metric+'_score'] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2d1002b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correctness_score    1.000000\n",
       "relevance_score      1.000000\n",
       "coherence_score      1.000000\n",
       "conciseness_score    1.000000\n",
       "latency              6.461154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['correctness_score','relevance_score','coherence_score','conciseness_score','latency']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808bdcf",
   "metadata": {},
   "source": [
    "# Query relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6b541f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_search_faiss(query, k=3, threshold=0.5):\n",
    "    query_vector = model.encode([query])[0].astype('float32')\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if dist < threshold:  # Only include results within the threshold distance\n",
    "            results.append({\n",
    "                'distance': dist,\n",
    "                'content': sections_data[idx]['content'],\n",
    "                'metadata': sections_data[idx]['metadata']\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4f579654",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in dietary guidelines. \n",
    "Use the following pieces of context to answer the question concisely. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def new_answer_question(query):\n",
    "    # Search for relevant context\n",
    "    search_results = new_search_faiss(query)\n",
    "    \n",
    "    if result==[]:\n",
    "        response=\"I don't know\"\n",
    "    else:\n",
    "        context = \"\\n\\n\".join([result['content'] for result in search_results])\n",
    "        response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1f83ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2=irr_q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f06474e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 62.71it/s]\n"
     ]
    }
   ],
   "source": [
    "time_list=[]\n",
    "response_list=[]\n",
    "for i in tqdm(range(len(irr_q2))):\n",
    "    query = irr_q['Questions'].values[i]\n",
    "    start = time.time()\n",
    "    response = new_answer_question(query)\n",
    "    end = time.time()   \n",
    "    time_list.append(end-start)\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "52db6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2['response']=response_list\n",
    "irr_q2['latency']=time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4508de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_q2['irrelevant_score'] = irr_q2['response'].str.contains(\"I don't know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3d34ba06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "irrelevant_score    1.000000\n",
       "latency             0.015869\n",
       "dtype: float64"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irr_q2[['irrelevant_score','latency']].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
